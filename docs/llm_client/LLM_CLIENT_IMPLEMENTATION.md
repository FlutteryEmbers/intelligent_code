# LLM Client å®ç°å®Œæˆ

## ğŸ‰ æ–°å¢åŠŸèƒ½

å·²åœ¨ç°æœ‰é¡¹ç›®éª¨æ¶ä¸ŠæˆåŠŸå®ç°æœ¬åœ° LLM è°ƒç”¨å°è£…ï¼

## ğŸ“¦ æ–°å¢/ä¿®æ”¹çš„æ–‡ä»¶

### æ ¸å¿ƒå®ç°

1. **[src/engine/llm_client.py](../src/engine/llm_client.py)** â­ æ–°å¢
   - `LLMClient` ç±»ï¼šå®Œæ•´çš„ LLM è°ƒç”¨å°è£…
   - `generate_training_sample()` æ–¹æ³•ï¼šç»“æ„åŒ–è¾“å‡ºç”Ÿæˆ
   - è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š 2 æ¬¡ï¼‰
   - å¤±è´¥è®°å½•åˆ° `rejected_llm.jsonl`
   - å†…ç½®è‡ªæµ‹ä»£ç ï¼ˆ`if __name__ == "__main__":`ï¼‰

2. **[src/utils/logger.py](../src/utils/logger.py)** â­ æ–°å¢
   - `LoggerManager` ç±»ï¼šç»Ÿä¸€æ—¥å¿—ç®¡ç†
   - `get_logger()` å‡½æ•°ï¼šä¾¿æ·çš„æ—¥å¿—å™¨è·å–
   - æ”¯æŒæ–‡ä»¶å’Œæ§åˆ¶å°åŒè¾“å‡º

3. **[test_llm_client.py](../test_llm_client.py)** â­ æ–°å¢
   - ç‹¬ç«‹çš„æµ‹è¯•è„šæœ¬
   - 5 æ­¥å®Œæ•´æµ‹è¯•æµç¨‹
   - å‹å¥½çš„æµ‹è¯•æŠ¥å‘Šè¾“å‡º

### é…ç½®æ›´æ–°

4. **[src/engine/__init__.py](../src/engine/__init__.py)** ğŸ”§ ä¿®æ”¹
   - å¯¼å‡º `LLMClient` ç±»

5. **[src/utils/__init__.py](../src/utils/__init__.py)** ğŸ”§ ä¿®æ”¹
   - å¯¼å‡º `get_logger` å’Œ `LoggerManager`

6. **[configs/pipeline.yaml](../configs/pipeline.yaml)** ğŸ”§ ä¿®æ”¹
   - æ›´æ–° `llm.base_url` ä¸º `http://localhost:11434/v1`ï¼ˆOpenAI å…¼å®¹ç«¯ç‚¹ï¼‰
   - æ›´æ–° `llm.model` ä¸º `qwen2.5-coder-3b-instruct`

7. **[src/utils/config.py](../src/utils/config.py)** ğŸ”§ ä¿®æ”¹
   - æ”¯æŒ `LLM_TIMEOUT` ç¯å¢ƒå˜é‡
   - `base_url` è‡ªåŠ¨æ·»åŠ  `/v1` åç¼€
   - æ›´æ–°é»˜è®¤æ¨¡å‹åç§°

8. **[.env.example](../.env.example)** ğŸ”§ ä¿®æ”¹
   - æ›´æ–° LLM é…ç½®é¡¹
   - æ·»åŠ è¯¦ç»†æ³¨é‡Š

### æ–‡æ¡£

9. **[docs/LLM_CLIENT_GUIDE.md](LLM_CLIENT_GUIDE.md)** â­ æ–°å¢
   - å®Œæ•´çš„ä½¿ç”¨æŒ‡å—
   - API å‚è€ƒ
   - æ•…éšœæ’æŸ¥
   - æœ€ä½³å®è·µ

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### 1. ç»“æ„åŒ–è¾“å‡º

ä½¿ç”¨ `PydanticOutputParser` å¼ºåˆ¶ LLM è¾“å‡ºç¬¦åˆ `TrainingSample` schemaï¼š

```python
from src.engine import LLMClient

client = LLMClient()
sample = client.generate_training_sample(
    system_prompt="ä½ æ˜¯ä¸€ä¸ª Java ä»£ç åˆ†æä¸“å®¶",
    user_prompt="åˆ†æè¿™æ®µä»£ç ...",
    scenario="qa_rule",
    repo_commit="abc123"
)

# sample è‡ªåŠ¨éªŒè¯ä¸º TrainingSample å¯¹è±¡
print(sample.instruction)
print(sample.answer)
```

### 2. è‡ªåŠ¨é‡è¯•æœºåˆ¶

```
å°è¯• 1: åŸå§‹æç¤ºè¯
  â†“ å¤±è´¥ï¼ˆJSON è§£æé”™è¯¯ï¼‰
å°è¯• 2: å¼ºåŒ–æç¤ºï¼š"åªè¾“å‡ºåˆæ³• JSONï¼Œä¸è¦é¢å¤–æ–‡å­—"
  â†“ å¤±è´¥ï¼ˆValidationErrorï¼‰
å°è¯• 3: å†æ¬¡å¼ºåŒ–æç¤º
  â†“ å¤±è´¥
è®°å½•åˆ° rejected_llm.jsonl + æŠ›å‡º ValueError
```

### 3. å¤±è´¥æ ·æœ¬è®°å½•

æ‰€æœ‰æ— æ³•è§£æçš„è¾“å‡ºéƒ½ä¼šè®°å½•åˆ° `data/intermediate/rejected_llm.jsonl`ï¼š

```json
{
  "timestamp": "2026-01-03T10:30:00Z",
  "system_prompt": "...",
  "user_prompt": "...",
  "raw_output": "æ¨¡å‹çš„å®é™…è¾“å‡º",
  "error": "ValidationError: Field required: 'instruction'",
  "model": "qwen2.5-coder-3b-instruct",
  "temperature": 0.7
}
```

### 4. å®Œæ•´æ—¥å¿—

```python
from src.utils import get_logger

logger = get_logger(__name__)
logger.info("Processing started")
logger.error("An error occurred", exc_info=True)
```

æ—¥å¿—ä¼šåŒæ—¶è¾“å‡ºåˆ°ï¼š
- æ–‡ä»¶ï¼š`logs/pipeline.log`
- æ§åˆ¶å°ï¼š`stdout`

### 5. çµæ´»é…ç½®

æ”¯æŒ 3 ç§é…ç½®æ–¹å¼ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š

```python
# 1. æ„é€ å‡½æ•°å‚æ•°
client = LLMClient(
    model="custom-model",
    temperature=0.5
)

# 2. ç¯å¢ƒå˜é‡
# export OLLAMA_MODEL=qwen2.5-coder-3b-instruct

# 3. é…ç½®æ–‡ä»¶
# configs/pipeline.yaml
```

## ğŸš€ å¿«é€Ÿæµ‹è¯•

### æ–¹å¼ 1ï¼šè¿è¡Œæ¨¡å—è‡ªæµ‹

```bash
python -m src.engine.llm_client
```

### æ–¹å¼ 2ï¼šä½¿ç”¨ç‹¬ç«‹æµ‹è¯•è„šæœ¬

```bash
python test_llm_client.py
```

### æ–¹å¼ 3ï¼šPython äº¤äº’å¼æµ‹è¯•

```python
from src.engine import LLMClient

client = LLMClient()
client.test_connection()  # æµ‹è¯•è¿æ¥
```

## ğŸ“‹ æµ‹è¯•å‰å‡†å¤‡

### 1. å®‰è£…ä¾èµ–ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰

```bash
pip install -r requirements.txt
```

### 2. å¯åŠ¨ Ollama æœåŠ¡

```bash
ollama serve
```

### 3. æ‹‰å–æ¨¡å‹

```bash
# æ–¹å¼ 1ï¼šä½¿ç”¨æ¨èçš„å°æ¨¡å‹ï¼ˆ3Bï¼‰
ollama pull qwen2.5-coder-3b-instruct

# æ–¹å¼ 2ï¼šä½¿ç”¨å…¶ä»–æ¨¡å‹
ollama pull qwen2.5:7b
```

### 4. éªŒè¯æœåŠ¡

```bash
curl http://localhost:11434/v1/models
```

åº”è¯¥è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨ã€‚

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•

```python
from src.engine import LLMClient

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = LLMClient()

# æ„å»ºæç¤ºè¯
system_prompt = "ä½ æ˜¯ä¸€ä¸ª Java ä»£ç åˆ†æä¸“å®¶"
user_prompt = """
åˆ†æä»¥ä¸‹ä»£ç å¹¶ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼š

```java
public class Example {
    public void hello() {
        System.out.println("Hello");
    }
}
```
"""

# ç”Ÿæˆæ ·æœ¬
try:
    sample = client.generate_training_sample(
        system_prompt=system_prompt,
        user_prompt=user_prompt,
        scenario="qa_rule",
        repo_commit="abc123"
    )
    
    print(f"âœ“ æ ·æœ¬ç”ŸæˆæˆåŠŸ")
    print(f"  Instruction: {sample.instruction}")
    print(f"  Answer: {sample.answer[:100]}...")
    
except ValueError as e:
    print(f"âœ— ç”Ÿæˆå¤±è´¥: {e}")
```

### æ‰¹é‡ç”Ÿæˆ

```python
from tqdm import tqdm

code_snippets = [...]  # ä»£ç ç‰‡æ®µåˆ—è¡¨
samples = []

for snippet in tqdm(code_snippets, desc="ç”Ÿæˆæ ·æœ¬"):
    try:
        sample = client.generate_training_sample(
            system_prompt=system_prompt,
            user_prompt=f"åˆ†æä»£ç ï¼š\n{snippet}",
            scenario="qa_rule",
            repo_commit="abc123"
        )
        samples.append(sample)
    except ValueError:
        continue  # è·³è¿‡å¤±è´¥çš„æ ·æœ¬

print(f"æˆåŠŸç”Ÿæˆ {len(samples)} ä¸ªæ ·æœ¬")
```

### è‡ªå®šä¹‰é…ç½®

```python
# ä½¿ç”¨æ›´ä½çš„ temperature æé«˜ä¸€è‡´æ€§
client = LLMClient(temperature=0.3)

# ä½¿ç”¨æ›´å¤§çš„ max_tokens å…è®¸æ›´é•¿çš„è¾“å‡º
client = LLMClient(max_tokens=4000)

# ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
client = LLMClient(model="qwen2.5:7b")
```

## ğŸ” API æ¥å£

### LLMClient

```python
class LLMClient:
    def __init__(
        self,
        base_url: str | None = None,      # Ollama API åœ°å€
        model: str | None = None,          # æ¨¡å‹åç§°
        temperature: float | None = None,  # æ¸©åº¦å‚æ•°
        max_tokens: int | None = None,     # æœ€å¤§ token æ•°
        timeout: int | None = None,        # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    )
    
    def generate_training_sample(
        self,
        system_prompt: str,      # ç³»ç»Ÿæç¤ºè¯
        user_prompt: str,        # ç”¨æˆ·æç¤ºè¯
        scenario: str = "qa_rule",           # åœºæ™¯ç±»å‹
        repo_commit: str = "unknown"         # ä»“åº“ commit
    ) -> TrainingSample
    
    def test_connection(self) -> bool
```

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šè¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥ Ollama æœåŠ¡
ollama serve

# æµ‹è¯•è¿æ¥
curl http://localhost:11434/v1/models
```

### é—®é¢˜ 2ï¼šæ¨¡å‹æœªæ‰¾åˆ°

```bash
# åˆ—å‡ºå·²å®‰è£…çš„æ¨¡å‹
ollama list

# æ‹‰å–æ‰€éœ€æ¨¡å‹
ollama pull qwen2.5-coder-3b-instruct
```

### é—®é¢˜ 3ï¼šè¾“å‡ºæ ¼å¼é”™è¯¯

æŸ¥çœ‹ `data/intermediate/rejected_llm.jsonl` äº†è§£è¯¦æƒ…ï¼š

```bash
# Windows PowerShell
Get-Content data/intermediate/rejected_llm.jsonl | Select-Object -Last 1 | ConvertFrom-Json

# Linux/Mac
tail -n 1 data/intermediate/rejected_llm.jsonl | jq .
```

### é—®é¢˜ 4ï¼šPython æ¨¡å—å¯¼å…¥é”™è¯¯

```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd d:\Codes\intelligent_code_generator

# æ£€æŸ¥ Python è·¯å¾„
python -c "import sys; print('\n'.join(sys.path))"
```

## ğŸ“Š è¾“å‡ºç¤ºä¾‹

### æˆåŠŸçš„æ ·æœ¬

```json
{
  "scenario": "qa_rule",
  "instruction": "è¿™ä¸ª Calculator ç±»æä¾›äº†å“ªäº›æ•°å­¦è¿ç®—åŠŸèƒ½ï¼Ÿ",
  "context": "public class Calculator { ... }",
  "thought": {
    "observations": ["ç±»ä¸­å®šä¹‰äº† add å’Œ subtract æ–¹æ³•"],
    "inferences": ["æä¾›äº†åŸºæœ¬çš„åŠ å‡è¿ç®—"],
    "evidence_refs": [],
    "assumptions": ["æ–¹æ³•æ˜¯å…¬å¼€çš„"]
  },
  "answer": "è¯¥ Calculator ç±»æä¾›äº†ä¸¤ä¸ªåŸºæœ¬æ•°å­¦è¿ç®—åŠŸèƒ½ï¼šadd() ç”¨äºåŠ æ³•ï¼Œsubtract() ç”¨äºå‡æ³•ã€‚",
  "repo_commit": "abc123",
  "quality": {},
  "created_at": "2026-01-03T10:30:00.000000+00:00",
  "sample_id": "1a2b3c4d5e6f7g8h"
}
```

### æ‹’ç»çš„æ ·æœ¬

```json
{
  "timestamp": "2026-01-03T10:30:00Z",
  "system_prompt": "ä½ æ˜¯ä¸€ä¸ª Java ä»£ç åˆ†æä¸“å®¶...",
  "user_prompt": "åˆ†æä»£ç ...",
  "raw_output": "è¿™æ˜¯ä¸€ä¸ªè®¡ç®—å™¨ç±»...",  # ä¸æ˜¯ JSON
  "error": "JSONDecodeError: Expecting value: line 1 column 1",
  "model": "qwen2.5-coder-3b-instruct",
  "temperature": 0.7
}
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

| æ“ä½œ | é¢„æœŸæ—¶é—´ |
|------|---------|
| åˆå§‹åŒ–å®¢æˆ·ç«¯ | < 1 ç§’ |
| æµ‹è¯•è¿æ¥ | 1-3 ç§’ |
| ç”Ÿæˆå•ä¸ªæ ·æœ¬ï¼ˆ3B æ¨¡å‹ï¼‰ | 5-15 ç§’ |
| ç”Ÿæˆå•ä¸ªæ ·æœ¬ï¼ˆ7B æ¨¡å‹ï¼‰ | 10-30 ç§’ |

## ğŸ¯ ä¸‹ä¸€æ­¥

1. âœ… LLM è°ƒç”¨å°è£…ï¼ˆå·²å®Œæˆï¼‰
2. â¬œ å®ç° `JavaParser`ï¼ˆåŸºäº tree-sitter-javaï¼‰
3. â¬œ å®ç° `SampleGenerator`ï¼ˆç¼–æ’ Parser + LLMClientï¼‰
4. â¬œ å®ç° `QualityChecker`ï¼ˆè´¨é‡è¯„ä¼°å’Œå»é‡ï¼‰
5. â¬œ å®Œæ•´çš„ç«¯åˆ°ç«¯ç®¡é“

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **[LLM Client ä½¿ç”¨æŒ‡å—](LLM_CLIENT_GUIDE.md)** - è¯¦ç»†çš„ API æ–‡æ¡£å’Œæœ€ä½³å®è·µ
- **[é¡¹ç›®ç»“æ„](../STRUCTURE.md)** - é¡¹ç›®æ¶æ„è¯´æ˜
- **[å¿«é€Ÿå‚è€ƒ](../QUICKREF.md)** - å¸¸ç”¨å‘½ä»¤å’Œ API
- **[å®‰è£…æŒ‡å—](../INSTALL.md)** - ç¯å¢ƒé…ç½®

## âœ… éªŒè¯æ¸…å•

- [x] `LLMClient` ç±»å®ç°
- [x] `generate_training_sample()` æ–¹æ³•
- [x] Pydantic è¾“å‡ºè§£æå™¨é›†æˆ
- [x] è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š 2 æ¬¡ï¼‰
- [x] å¤±è´¥æ ·æœ¬è®°å½•åˆ° `rejected_llm.jsonl`
- [x] æ—¥å¿—å·¥å…·å®ç°
- [x] é…ç½®ç®¡ç†æ›´æ–°
- [x] ç¯å¢ƒå˜é‡æ”¯æŒ
- [x] è‡ªæµ‹ä»£ç 
- [x] ç‹¬ç«‹æµ‹è¯•è„šæœ¬
- [x] å®Œæ•´æ–‡æ¡£

---

**å®ç°å®Œæˆï¼** ğŸ‰

ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹ä½¿ç”¨ `LLMClient` ç”Ÿæˆè®­ç»ƒæ ·æœ¬äº†ã€‚
