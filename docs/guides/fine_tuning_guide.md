# Fine-Tuning Module Guide

æœ¬æ¨¡å—æä¾›äº†åŸºäº LoRA/QLoRA çš„å¤§æ¨¡å‹å¾®è°ƒä¸è¯„ä¼°çš„ä¸€ç«™å¼å·¥å…·é“¾ï¼Œæ”¯æŒ SFTï¼ˆæœ‰ç›‘ç£å¾®è°ƒï¼‰å’ŒåŸºäº ROUGE/BLEU çš„è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚

## ğŸ“‚ æ¨¡å—ç»“æ„ (`fine_tuning/`)

*   **`train.py`**: å¾®è°ƒä¸»å…¥å£ï¼ŒåŸºäº HuggingFace Trainerã€‚
*   **`eval.py`**: è¯„ä¼°ä¸»å…¥å£ï¼Œæ”¯æŒåŸºåº§æ¨¡å‹å¯¹æ¯”ã€æŒ‡æ ‡è®¡ç®—ä¸å®šæ€§æŠ¥å‘Šç”Ÿæˆã€‚
*   **`download_model.py`**: è¾…åŠ©å·¥å…·ï¼Œç”¨äºä¸‹è½½ HF æ¨¡å‹æƒé‡ã€‚
*   **`configs/*.yaml`**: è®­ç»ƒé…ç½®æ–‡ä»¶ï¼ˆåŒ…å« QLoRA å‚æ•°ã€è·¯å¾„é…ç½®ç­‰ï¼‰ã€‚
*   **`libs/`**: æ ¸å¿ƒé€»è¾‘å°è£…ï¼ˆDataset Loading, Trainer æ‰©å±•ç­‰ï¼‰ã€‚

## âš™ï¸ ç¯å¢ƒå‡†å¤‡

å»ºè®®ä¸ºè®­ç»ƒå•ç‹¬åˆ›å»ºä¸€ä¸ªç¯å¢ƒï¼Œé¿å…ä¸ä¸» Pipeline å†²çªï¼š

```bash
conda create -n finetune python=3.10
conda activate finetune
pip install torch transformers peft datasets accelerate bitsandbytes rouge_score nltk pyyaml
# Windows ç”¨æˆ·è¯·æ³¨æ„å®‰è£…é€‚åˆçš„ Torch CUDA ç‰ˆæœ¬
```

## ğŸš€ è¿è¡ŒæŒ‡å—

### 1. ä¸‹è½½æ¨¡å‹æƒé‡ (Optional)

å¦‚æœæœ¬åœ°æ²¡æœ‰åº•åº§æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨è„šæœ¬ä¸‹è½½ï¼ˆé»˜è®¤ä¸‹è½½åˆ° `fine_tuning/models/`ï¼‰ï¼š

```bash
python fine_tuning/download_model.py --model_name Qwen/Qwen2.5-Coder-1.5B-Instruct
```

### 2. å¯åŠ¨å¾®è°ƒ (Training)

ä½¿ç”¨ `configs/` ä¸‹çš„ YAML é…ç½®æ–‡ä»¶å¯åŠ¨è®­ç»ƒã€‚

**CMD ç¤ºä¾‹**:
```bash
# Windows / Linux
python fine_tuning/train.py fine_tuning/configs/lora_1.5b.yaml
```

**é…ç½®æ–‡ä»¶è¯´æ˜ (`lora_1.5b.yaml`)**:
```yaml
model_name: "Qwen/Qwen2.5-Coder-1.5B-Instruct"  # åº•åº§æ¨¡å‹è·¯å¾„
data_path: "../assets/data/final"                 # è®­ç»ƒæ•°æ®è·¯å¾„
output_dir: "./checkpoints/lora-1.5b"             # Checkpoint ä¿å­˜è·¯å¾„
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2e-4
  num_train_epochs: 3
  use_lora: true                                  # å¼€å¯ LoRA
  use_qlora: true                                 # å¼€å¯ 4-bit é‡åŒ–
```

### 3. æ¨¡å‹è¯„ä¼° (Evaluation)

è¯„ä¼°è„šæœ¬ä¼šè‡ªåŠ¨åŠ è½½è®­ç»ƒå¥½çš„ Adapterï¼Œå¹¶ä¸åŸºåº§æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚

**CMD ç¤ºä¾‹**:
```bash
# è‡ªåŠ¨è¯»å– config ä¸­çš„ output_dir å¯»æ‰¾ checkpoint
python fine_tuning/eval.py --config fine_tuning/configs/lora_1.5b.yaml --compare-base --report
```

**å‚æ•°è¯´æ˜**:
- `--config`: æŒ‡å®šè®­ç»ƒæ—¶çš„é…ç½®æ–‡ä»¶ï¼ˆç”¨äºè‡ªåŠ¨å®šä½ checkpoint å’Œæ•°æ®ï¼‰ã€‚
- `--compare-base`: æ˜¯å¦åŒæ—¶è¿è¡ŒåŸºåº§æ¨¡å‹ï¼ˆBase Modelï¼‰è¿›è¡Œå¯¹æ¯”ã€‚
- `--report`: ç”Ÿæˆ Markdown æ ¼å¼çš„è¯¦ç»†æŠ¥å‘Šï¼ˆåŒ…å« Metrics è¡¨æ ¼å’Œå®šæ€§ Caseï¼‰ã€‚

**è¾“å‡ºç»“æœ**:
- è¯„ä¼°æŠ¥å‘Š: `assets/eval_fine_tuning_report.md`
- è¯¦ç»†ç»“æœ: `assets/data/eval_results.jsonl`

## ğŸ“Š å¸¸è§é—®é¢˜

- **Windows å¤šè¿›ç¨‹æŠ¥é”™**: å¦‚æœé‡åˆ° `BrokenPipeError`ï¼Œè¯·åœ¨ yaml ä¸­è®¾ç½® `dataloader_num_workers: 0`ã€‚
- **æ˜¾å­˜ä¸è¶³ (OOM)**:
    - å‡å° `per_device_train_batch_size` (e.g., 1)ã€‚
    - å¢åŠ  `gradient_accumulation_steps` (e.g., 16)ã€‚
    - ç¡®ä¿ `use_qlora: true` ä»¥å¯ç”¨ 4-bit é‡åŒ–ã€‚
