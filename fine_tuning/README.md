# è®­ç»ƒæ¨¡å—

æœ¬ç›®å½•åŒ…å«åŸºäºé¡¹ç›®ç”Ÿæˆçš„ SFT æ•°æ®ï¼ˆ`../data/final/*_sft.jsonl`ï¼‰å¯¹ Qwen2.5-Coder ç³»åˆ—æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„å®Œæ•´è®­ç»ƒè„šæœ¬ã€‚

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

- âœ… LoRA/QLoRA å¾®è°ƒæ”¯æŒ
- âœ… ç‹¬ç«‹çš„ä¾èµ–ç®¡ç†ï¼ˆä¸å½±å“æ•°æ®ç”Ÿæˆç¯å¢ƒï¼‰
- âœ… å¤šç§æ¨¡å‹å°ºå¯¸é…ç½®ï¼ˆ1.5B / 3B / 7Bï¼‰
- âœ… è‡ªåŠ¨è¯„æµ‹å’Œäººå·¥è¯„æµ‹æ”¯æŒ
- âœ… Chat template è‡ªåŠ¨å¤„ç†
- âœ… è®­ç»ƒç›‘æ§ï¼ˆwandb/tensorboardï¼‰

## ğŸ“‹ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# è¿›å…¥è®­ç»ƒæ¨¡å—ç›®å½•
cd fine_tuning

# å®‰è£…è®­ç»ƒä¾èµ–ï¼ˆå»ºè®®ä½¿ç”¨ç‹¬ç«‹ç¯å¢ƒï¼‰
pip install -r requirements.txt
```

### 2. ä¸‹è½½åº•åº§æ¨¡å‹

```bash
# ä½¿ç”¨å¿«æ·æ–¹å¼ä¸‹è½½ï¼ˆæ¨èï¼‰
python download_model.py --model 1.5b
python download_model.py --model 3b
python download_model.py --model 7b

# æˆ–æ‰‹åŠ¨ä¸‹è½½
huggingface-cli download Qwen/Qwen2.5-Coder-1.5B-Instruct \
  --local-dir ../models/Qwen2.5-Coder-1.5B-Instruct \
  --local-dir-use-symlinks False
```

### 3. å¯åŠ¨è®­ç»ƒ

```bash
# LoRA è®­ç»ƒï¼ˆæ¨èå…ˆç”¨ 1.5B å¿«é€ŸéªŒè¯ï¼‰
python train.py configs/lora_1.5b.yaml

# ä½¿ç”¨æ›´å¤§æ¨¡å‹
python train.py configs/lora_3b.yaml

# QLoRA è®­ç»ƒï¼ˆæ˜¾å­˜å—é™æ—¶ï¼‰
python train.py configs/qlora_7b.yaml
```

### 4. è¯„æµ‹æ¨¡å‹

```bash
# ä½¿ç”¨é»˜è®¤è·¯å¾„è¯„æµ‹ï¼ˆæœ€ç®€å•ï¼‰
python eval.py

# æˆ–è‡ªå®šä¹‰è·¯å¾„
python eval.py \
  --checkpoint ../checkpoints/lora-qwen2.5-coder-3b \
  --data ../data/final/test_sft.jsonl

# å¿«é€Ÿæµ‹è¯•ï¼ˆåªè¯„æµ‹10ä¸ªæ ·æœ¬ï¼‰
python eval.py --max-samples 10
```

**é»˜è®¤è·¯å¾„è¯´æ˜**ï¼š
- Checkpoint: `../checkpoints/lora-qwen2.5-coder-1.5b`
- éªŒè¯æ•°æ®: `../data/final/val_sft.jsonl`
- è¾“å‡ºç»“æœ: `../data/eval_results.jsonl`

## ğŸ“ ç›®å½•ç»“æ„

```
fine_tuning/
â”œâ”€â”€ README.md                # æœ¬æ–‡æ¡£
â”œâ”€â”€ requirements.txt         # è®­ç»ƒä¾èµ–
â”‚
â”œâ”€â”€ train.py                 # â† è®­ç»ƒå…¥å£ï¼ˆç”¨æˆ·å·¥å…·ï¼‰
â”œâ”€â”€ eval.py                  # â† è¯„æµ‹å…¥å£
â”œâ”€â”€ download_model.py        # â† æ¨¡å‹ä¸‹è½½
â”‚
â”œâ”€â”€ configs/                 # è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ lora_1.5b.yaml
â”‚   â”œâ”€â”€ lora_3b.yaml
â”‚   â””â”€â”€ qlora_7b.yaml
â”‚
â”œâ”€â”€ libs/                    # æ ¸å¿ƒåº“ï¼ˆå¯å¤ç”¨æ¨¡å—ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py           # è®­ç»ƒé€»è¾‘
â”‚   â”œâ”€â”€ data_loader.py       # æ•°æ®åŠ è½½
â”‚   â””â”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”‚
â””â”€â”€ evaluation/              # è¯„æµ‹æ¨¡å—ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ evaluator.py
    â”œâ”€â”€ metrics.py
    â””â”€â”€ human_eval.py
```

## âš™ï¸ é…ç½®è¯´æ˜

è®­ç»ƒé…ç½®æ–‡ä»¶ä½äº `configs/` ç›®å½•ï¼Œä¸»è¦å‚æ•°ï¼š

- `base_model`: åº•åº§æ¨¡å‹è·¯å¾„
- `output_dir`: è®­ç»ƒè¾“å‡ºè·¯å¾„
- `train_data` / `val_data`: è®­ç»ƒ/éªŒè¯æ•°æ®è·¯å¾„
- `lora_r` / `lora_alpha`: LoRA å‚æ•°
- `max_seq_length`: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆä»£ç ä¸Šä¸‹æ–‡è¾ƒé•¿ï¼Œå»ºè®® 4096+ï¼‰
- `learning_rate`: å­¦ä¹ ç‡ï¼ˆLoRA å»ºè®® 1e-4ï¼‰
- `num_train_epochs`: è®­ç»ƒè½®æ•°

è¯¦ç»†é…ç½®è¯´æ˜è§ `../docs/guides/training_guide.md`

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A: å°è¯•ä»¥ä¸‹æ–¹æ¡ˆï¼š
1. ä½¿ç”¨ QLoRAï¼ˆ4bit é‡åŒ–ï¼‰
2. å‡å° `per_device_train_batch_size`ï¼Œå¢å¤§ `gradient_accumulation_steps`
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆ1.5B â†’ 3Bï¼‰
4. å‡å° `max_seq_length`

### Q: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

A: ä¼˜åŒ–å»ºè®®ï¼š
1. å¯ç”¨ flash attentionï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰
2. å¢å¤§ batch sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
3. ä½¿ç”¨å¤šå¡è®­ç»ƒï¼ˆä¿®æ”¹è„šæœ¬æ·»åŠ  `accelerate launch`ï¼‰

### Q: å¦‚ä½•æŸ¥çœ‹è®­ç»ƒè¿›åº¦ï¼Ÿ

A: ä¸‰ç§æ–¹å¼ï¼š
1. wandbï¼šåœ¨é…ç½®ä¸­å¯ç”¨ `report_to: wandb`
2. tensorboardï¼š`tensorboard --logdir ../checkpoints/xxx`
3. æ—¥å¿—æ–‡ä»¶ï¼š`tail -f ../logs/training.log`

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [å¾®è°ƒæŠ€æœ¯æ–¹æ¡ˆ](../docs/guides/fine_tuning_guide.md) - æ¨¡å‹é€‰å‹å’Œè®­ç»ƒç­–ç•¥
- [è®­ç»ƒæ“ä½œæ‰‹å†Œ](../docs/guides/training_guide.md) - è¯¦ç»†æ“ä½œæ­¥éª¤
- [è¯„æµ‹æŒ‡å—](../docs/guides/evaluation_guide.md) - å¦‚ä½•è¯„æµ‹æ¨¡å‹æ•ˆæœ

## ğŸ¤ è´¡çŒ®

è®­ç»ƒæ¨¡å—ç‹¬ç«‹ç»´æŠ¤ï¼Œæ¬¢è¿æäº¤ PR æ”¹è¿›è®­ç»ƒè„šæœ¬å’Œé…ç½®ã€‚
