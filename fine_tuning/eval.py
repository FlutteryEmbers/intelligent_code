#!/usr/bin/env python3
"""
æ¨¡å‹è¯„æµ‹è„šæœ¬

Usage:
    python eval.py                                    # ä½¿ç”¨é»˜è®¤è·¯å¾„
    python eval.py --checkpoint ../checkpoints/xxx    # è‡ªå®šä¹‰checkpoint
    python eval.py --max-samples 10                   # åªè¯„æµ‹10ä¸ªæ ·æœ¬
"""
import argparse
import json
import yaml
import logging
import sys
from pathlib import Path
from typing import List, Dict

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from tqdm import tqdm

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_model_and_tokenizer(checkpoint_dir: str, base_model_path: str = None, only_base: bool = False):
    """åŠ è½½æ¨¡å‹å’Œ tokenizerï¼ˆæ”¯æŒ LoRA adapterï¼‰"""
    checkpoint_path = Path(checkpoint_dir).resolve()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA checkpoint
    adapter_config = checkpoint_path / "adapter_config.json"
    is_lora = adapter_config.exists()
    
    if is_lora:
        logger.info("Loading LoRA adapter...")
        # ä» adapter_config è¯»å– base model è·¯å¾„
        with open(adapter_config) as f:
            config = json.load(f)
            base_model_path = base_model_path or config.get("base_model_name_or_path")
        
        # è§£æ base model è·¯å¾„
        if base_model_path:
            base_model_path_obj = Path(base_model_path)
            # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            if base_model_path_obj.exists():
                base_model_path = str(base_model_path_obj.resolve())
        
        logger.info(f"Base model: {base_model_path}")
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
        is_local_base = Path(base_model_path).exists()
        
        if only_base:
            logger.info(f"Loading BASE model (no adapter): {base_model_path}")
            # åŠ è½½ base model
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                dtype=torch.bfloat16,
                device_map="auto",
                local_files_only=is_local_base
            )
            tokenizer = AutoTokenizer.from_pretrained(
                base_model_path,
                local_files_only=is_local_base
            )
            model.eval()
            return model, tokenizer

        # åŠ è½½ base model
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            dtype=torch.bfloat16,
            device_map="auto",
            local_files_only=is_local_base
        )
        
        # åŠ è½½ LoRA adapter
        model = PeftModel.from_pretrained(base_model, str(checkpoint_path))
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            local_files_only=is_local_base
        )
    else:
        logger.info("Loading full model...")
        model = AutoModelForCausalLM.from_pretrained(
            str(checkpoint_path),
            dtype=torch.bfloat16,
            device_map="auto",
            local_files_only=True
        )
        tokenizer = AutoTokenizer.from_pretrained(
            str(checkpoint_path),
            local_files_only=True
        )
    
    model.eval()
    return model, tokenizer


def load_eval_data(data_path: str) -> List[Dict]:
    """åŠ è½½è¯„æµ‹æ•°æ®"""
    data_path = Path(data_path).resolve()
    samples = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    return samples


def generate_response(model, tokenizer, messages: List[Dict], max_new_tokens: int = 1024) -> str:
    """ç”Ÿæˆå›å¤"""
    # åº”ç”¨ chat template
    prompt = tokenizer.apply_chat_template(
        messages[:-1],  # ä¸åŒ…å« assistant çš„æ¶ˆæ¯
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )
    
    # è§£ç ï¼ˆåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response


from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
import nltk

# Ensure punkt resources are downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

def compute_metrics(predictions: List[str], references: List[str]) -> Dict:
    """è®¡ç®—è¯„æµ‹æŒ‡æ ‡ï¼ˆåŒ…æ‹¬ Exact Match, BLEU, Rougeï¼‰"""
    total = len(predictions)
    if total == 0:
        return {}

    # Exact Match
    exact_matches = sum(1 for pred, ref in zip(predictions, references) if pred.strip() == ref.strip())
    
    # Rouge
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
    
    # BLEU
    bleu_score = 0.0
    smoothing = SmoothingFunction().method1
    
    for pred, ref in zip(predictions, references):
        # Rouge
        scores = scorer.score(ref, pred)
        for key in rouge_scores:
            rouge_scores[key] += scores[key].fmeasure
            
        # BLEU (simple tokenizer)
        ref_tokens = nltk.word_tokenize(ref)
        pred_tokens = nltk.word_tokenize(pred)
        bleu_score += sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)
    
    # Average scores
    metrics = {
        "total_samples": total,
        "exact_match_rate": exact_matches / total,
        "bleu": bleu_score / total,
    }
    for key in rouge_scores:
        metrics[key] = rouge_scores[key] / total
        
    return metrics


def evaluate(model, tokenizer, eval_data: List[Dict], output_file: str = None, max_samples: int = None):
    """æ‰§è¡Œè¯„æµ‹"""
    if max_samples:
        eval_data = eval_data[:max_samples]
    
    logger.info(f"Evaluating on {len(eval_data)} samples...")
    
    predictions = []
    references = []
    results = []
    
    for sample in tqdm(eval_data, desc="Evaluating"):
        messages = sample["messages"]
        
        # æå–referenceï¼ˆæœ€åä¸€æ¡assistantæ¶ˆæ¯ï¼‰
        reference = messages[-1]["content"] if messages[-1]["role"] == "assistant" else ""
        
        # ç”Ÿæˆé¢„æµ‹
        prediction = generate_response(model, tokenizer, messages)
        
        predictions.append(prediction)
        references.append(reference)
        
        # è®°å½•ç»“æœ
        results.append({
            "messages": messages,
            "prediction": prediction,
            "reference": reference
        })
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = compute_metrics(predictions, references)
    
    logger.info("\nğŸ“Š Evaluation Metrics:")
    for key, value in metrics.items():
        logger.info(f"  {key}: {value}")
    
    # ä¿å­˜ç»“æœ
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for result in results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        
        logger.info(f"\nğŸ’¾ Results saved to: {output_path}")
    
    return metrics, results


def main():
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆfine_tuningç›®å½•ï¼‰
    script_dir = Path(__file__).parent
    # é¡¹ç›®æ ¹ç›®å½•ï¼ˆfine_tuningçš„çˆ¶ç›®å½•ï¼‰
    project_root = script_dir.parent
    
    parser = argparse.ArgumentParser(
        description="Evaluate fine-tuned model",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # ä½¿ç”¨é»˜è®¤è·¯å¾„è¯„æµ‹
  python eval.py
  
  # è‡ªå®šä¹‰checkpoint
  python eval.py --checkpoint ../checkpoints/lora-qwen2.5-coder-3b
  
  # åªè¯„æµ‹10ä¸ªæ ·æœ¬ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
  python eval.py --max-samples 10
        """
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Path to training config file (e.g. configs/lora_1.5b.yaml)"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint directory (overrides config)"
    )
    parser.add_argument(
        "--data",
        type=str,
        default=None,
        help="Path to evaluation data (default: ../data/final/val_sft.jsonl)"
    )
    parser.add_argument(
        "--base-model",
        type=str,
        default=None,
        help="Base model path (only needed if not in adapter_config.json)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output file for evaluation results (default: ../data/eval_results.jsonl)"
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="Maximum number of samples to evaluate"
    )
    
    parser.add_argument(
        "--no-compare-base",
        action="store_false",
        dest="compare_base",
        help="Disable comparison with base model",
        default=True
    )

    parser.add_argument(
        "--report",
        type=str,
        default=None,
        help="Output markdown report file (default: ../data/eval_report.md)"
    )
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº† configï¼Œä» config è¯»å– checkpoint è·¯å¾„
    config_checkpoint = None
    if args.config:
        config_path = Path(args.config)
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                # è§£æ output_dir
                output_dir = Path(config.get("output_dir", ""))
                base_dir = config_path.parent.parent
                if output_dir and not output_dir.is_absolute():
                    config_checkpoint = str((base_dir / output_dir).resolve())
                elif output_dir:
                    config_checkpoint = str(output_dir)
        else:
            logger.warning(f"Config file not found: {args.config}")

    # è®¾ç½®é»˜è®¤è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡æ˜¯ config ä¸­çš„è·¯å¾„ï¼Œæœ€åæ˜¯é»˜è®¤ç¡¬ç¼–ç è·¯å¾„ï¼‰
    # æ³¨æ„ï¼šcheckpoints åœ¨ fine_tuning/checkpoints ç›®å½•ä¸‹ï¼Œå³ script_dir / "checkpoints"
    default_checkpoint = (script_dir / "checkpoints" / "lora-qwen2.5-coder-1.5b").resolve()
    checkpoint_path = args.checkpoint or config_checkpoint or str(default_checkpoint)
    data_path = args.data or str((project_root / "assets" / "data" / "final" / "val_sft.jsonl").resolve())
    output_path = args.output or str((project_root / "assets" / "data" / "eval_results.jsonl").resolve())
    report_path = args.report or str((project_root / "assets" / "data" / "eval_report.md").resolve())
    
    logger.info(f"Checkpoint: {checkpoint_path}")
    logger.info(f"Data: {data_path}")
    logger.info(f"Output: {output_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(checkpoint_path).exists():
        logger.error(f"âŒ Checkpoint not found: {checkpoint_path}")
        logger.info("\nğŸ’¡ Hint: Train a model first using: python train.py configs/lora_1.5b.yaml")
        sys.exit(1)
    
    if not Path(data_path).exists():
        logger.error(f"âŒ Data file not found: {data_path}")
        sys.exit(1)
    
    # åŠ è½½æ¨¡å‹
    logger.info("Loading model...")
    model, tokenizer = load_model_and_tokenizer(
        checkpoint_path,
        args.base_model
    )
    
    # åŠ è½½è¯„æµ‹æ•°æ®
    logger.info("Loading evaluation data...")
    eval_data = load_eval_data(data_path)
    
    # è¯„æµ‹
    metrics, ft_results = evaluate(
        model=model,
        tokenizer=tokenizer,
        eval_data=eval_data,
        output_file=output_path,
        max_samples=args.max_samples
    )
    
    logger.info("\nâœ… Evaluation completed!")

    if args.compare_base:
        logger.info("\nğŸ”„ cleaning up to run BASE model evaluation...")
        del model
        torch.cuda.empty_cache()
        import gc
        gc.collect()
        
        logger.info("loading BASE model...")
        base_model, base_tokenizer = load_model_and_tokenizer(
            checkpoint_path,
            args.base_model,
            only_base=True
        )
        
        logger.info("\nğŸ“‰ Evaluating BASE model...")
        base_metrics, base_results = evaluate(
            model=base_model,
            tokenizer=base_tokenizer,
            eval_data=eval_data,
            output_file=None, # Don't overwrite main results
            max_samples=args.max_samples
        )
        
        logger.info("\nğŸ“Š Comparison (Fine-tuned vs Base):")
        print(f"{'Metric':<20} | {'Fine-tuned':<15} | {'Base':<15} | {'Diff':<10}")
        print("-" * 65)
        for key in metrics:
            if isinstance(metrics[key], (int, float)):
                ft_val = metrics[key]
                base_val = base_metrics.get(key, 0)
                diff = ft_val - base_val
                print(f"{key:<20} | {ft_val:<15.4f} | {base_val:<15.4f} | {diff:<+10.4f}")

        logger.info("\nğŸ“ Qualitative Comparison Examples:")
        num_examples = min(2, len(ft_results))
        for i in range(num_examples):
            ft_res = ft_results[i]
            base_res = base_results[i]
            
            # Extract question (last user message)
            messages = ft_res['messages']
            question = "N/A"
            for msg in reversed(messages):
                if msg['role'] == 'user':
                    question = msg['content']
                    break
            
            reference = ft_res['reference']
            ft_pred = ft_res['prediction']
            base_pred = base_res['prediction']
            
            print(f"\nExample {i+1}:")
            print(f"â“ Question:\n{question[:200]}..." if len(question) > 200 else f"â“ Question:\n{question}")
            print(f"\nğŸ“– Reference:\n{reference[:200]}..." if len(reference) > 200 else f"\nğŸ“– Reference:\n{reference}")
            print(f"\nğŸ¤– Fine-tuned Model:\n{ft_pred[:200]}..." if len(ft_pred) > 200 else f"\nğŸ¤– Fine-tuned Model:\n{ft_pred}")
        print("-" * 80)

        # Save Markdown Report
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Evaluation Report\n\n")
            f.write(f"**Checkpoint**: `{checkpoint_path}`\n")
            f.write(f"**Base Model**: `{args.base_model or 'Auto-detected'}`\n")
            f.write(f"**Data**: `{data_path}`\n\n")

            f.write("## ğŸ“Š Metrics Comparison\n\n")
            f.write("| Metric | Fine-tuned | Base | Diff |\n")
            f.write("| :--- | :--- | :--- | :--- |\n")
            for key in metrics:
                if isinstance(metrics[key], (int, float)):
                    ft_val = metrics[key]
                    base_val = base_metrics.get(key, 0)
                    diff = ft_val - base_val
                    f.write(f"| {key} | {ft_val:.4f} | {base_val:.4f} | {diff:+.4f} |\n")
            
            f.write("\n## ğŸ“ Qualitative Examples\n\n")
            for i in range(min(5, len(ft_results))):
                ft_res = ft_results[i]
                base_res = base_results[i]
                
                # Extract question
                messages = ft_res['messages']
                question = "N/A"
                for msg in reversed(messages):
                    if msg['role'] == 'user':
                        question = msg['content']
                        break
                
                f.write(f"### Example {i+1}\n\n")
                f.write(f"**â“ Question**:\n\n{question}\n\n")
                f.write(f"**ğŸ“– Reference**:\n\n{ft_res['reference']}\n\n")
                f.write(f"**ğŸ¤– Fine-tuned Model**:\n\n{ft_res['prediction']}\n\n")
                f.write(f"**ğŸ‘¶ Base Model**:\n\n{base_res['prediction']}\n\n")
                f.write("---\n\n")
        
        logger.info(f"\nğŸ“„ Markdown report saved to: {report_path}")


if __name__ == "__main__":
    main()
