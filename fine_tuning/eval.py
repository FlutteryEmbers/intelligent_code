#!/usr/bin/env python3
"""
æ¨¡å‹è¯„æµ‹è„šæœ¬

Usage:
    python eval.py                                    # ä½¿ç”¨é»˜è®¤è·¯å¾„
    python eval.py --checkpoint ../checkpoints/xxx    # è‡ªå®šä¹‰checkpoint
    python eval.py --max-samples 10                   # åªè¯„æµ‹10ä¸ªæ ·æœ¬
"""
import argparse
import json
import logging
import sys
from pathlib import Path
from typing import List, Dict

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from tqdm import tqdm

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_model_and_tokenizer(checkpoint_dir: str, base_model_path: str = None):
    """åŠ è½½æ¨¡å‹å’Œ tokenizerï¼ˆæ”¯æŒ LoRA adapterï¼‰"""
    checkpoint_path = Path(checkpoint_dir).resolve()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA checkpoint
    adapter_config = checkpoint_path / "adapter_config.json"
    is_lora = adapter_config.exists()
    
    if is_lora:
        logger.info("Loading LoRA adapter...")
        # ä» adapter_config è¯»å– base model è·¯å¾„
        with open(adapter_config) as f:
            config = json.load(f)
            base_model_path = base_model_path or config.get("base_model_name_or_path")
        
        # è§£æ base model è·¯å¾„
        if base_model_path:
            base_model_path_obj = Path(base_model_path)
            # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            if base_model_path_obj.exists():
                base_model_path = str(base_model_path_obj.resolve())
        
        logger.info(f"Base model: {base_model_path}")
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
        is_local_base = Path(base_model_path).exists()
        
        # åŠ è½½ base model
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            local_files_only=is_local_base
        )
        
        # åŠ è½½ LoRA adapter
        model = PeftModel.from_pretrained(base_model, str(checkpoint_path))
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            local_files_only=is_local_base
        )
    else:
        logger.info("Loading full model...")
        model = AutoModelForCausalLM.from_pretrained(
            str(checkpoint_path),
            torch_dtype=torch.bfloat16,
            device_map="auto",
            local_files_only=True
        )
        tokenizer = AutoTokenizer.from_pretrained(
            str(checkpoint_path),
            local_files_only=True
        )
    
    model.eval()
    return model, tokenizer


def load_eval_data(data_path: str) -> List[Dict]:
    """åŠ è½½è¯„æµ‹æ•°æ®"""
    data_path = Path(data_path).resolve()
    samples = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    return samples


def generate_response(model, tokenizer, messages: List[Dict], max_new_tokens: int = 1024) -> str:
    """ç”Ÿæˆå›å¤"""
    # åº”ç”¨ chat template
    prompt = tokenizer.apply_chat_template(
        messages[:-1],  # ä¸åŒ…å« assistant çš„æ¶ˆæ¯
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )
    
    # è§£ç ï¼ˆåªä¿ç•™ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response


def compute_metrics(predictions: List[str], references: List[str]) -> Dict:
    """è®¡ç®—è¯„æµ‹æŒ‡æ ‡ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æŒ‡æ ‡ï¼ˆBLEU, ROUGEç­‰ï¼‰
    total = len(predictions)
    exact_matches = sum(1 for pred, ref in zip(predictions, references) if pred.strip() == ref.strip())
    
    return {
        "total_samples": total,
        "exact_match": exact_matches,
        "exact_match_rate": exact_matches / total if total > 0 else 0
    }


def evaluate(model, tokenizer, eval_data: List[Dict], output_file: str = None, max_samples: int = None):
    """æ‰§è¡Œè¯„æµ‹"""
    if max_samples:
        eval_data = eval_data[:max_samples]
    
    logger.info(f"Evaluating on {len(eval_data)} samples...")
    
    predictions = []
    references = []
    results = []
    
    for sample in tqdm(eval_data, desc="Evaluating"):
        messages = sample["messages"]
        
        # æå–referenceï¼ˆæœ€åä¸€æ¡assistantæ¶ˆæ¯ï¼‰
        reference = messages[-1]["content"] if messages[-1]["role"] == "assistant" else ""
        
        # ç”Ÿæˆé¢„æµ‹
        prediction = generate_response(model, tokenizer, messages)
        
        predictions.append(prediction)
        references.append(reference)
        
        # è®°å½•ç»“æœ
        results.append({
            "messages": messages,
            "prediction": prediction,
            "reference": reference
        })
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = compute_metrics(predictions, references)
    
    logger.info("\nğŸ“Š Evaluation Metrics:")
    for key, value in metrics.items():
        logger.info(f"  {key}: {value}")
    
    # ä¿å­˜ç»“æœ
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for result in results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        
        logger.info(f"\nğŸ’¾ Results saved to: {output_path}")
    
    return metrics


def main():
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆfine_tuningç›®å½•ï¼‰
    script_dir = Path(__file__).parent
    # é¡¹ç›®æ ¹ç›®å½•ï¼ˆfine_tuningçš„çˆ¶ç›®å½•ï¼‰
    project_root = script_dir.parent
    
    parser = argparse.ArgumentParser(
        description="Evaluate fine-tuned model",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # ä½¿ç”¨é»˜è®¤è·¯å¾„è¯„æµ‹
  python eval.py
  
  # è‡ªå®šä¹‰checkpoint
  python eval.py --checkpoint ../checkpoints/lora-qwen2.5-coder-3b
  
  # åªè¯„æµ‹10ä¸ªæ ·æœ¬ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
  python eval.py --max-samples 10
        """
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint directory (default: ../checkpoints/lora-qwen2.5-coder-1.5b)"
    )
    parser.add_argument(
        "--data",
        type=str,
        default=None,
        help="Path to evaluation data (default: ../data/final/val_sft.jsonl)"
    )
    parser.add_argument(
        "--base-model",
        type=str,
        default=None,
        help="Base model path (only needed if not in adapter_config.json)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output file for evaluation results (default: ../data/eval_results.jsonl)"
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="Maximum number of samples to evaluate"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
    checkpoint_path = args.checkpoint or str((project_root / "checkpoints" / "lora-qwen2.5-coder-1.5b").resolve())
    data_path = args.data or str((project_root / "data" / "final" / "val_sft.jsonl").resolve())
    output_path = args.output or str((project_root / "data" / "eval_results.jsonl").resolve())
    
    logger.info(f"Checkpoint: {checkpoint_path}")
    logger.info(f"Data: {data_path}")
    logger.info(f"Output: {output_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(checkpoint_path).exists():
        logger.error(f"âŒ Checkpoint not found: {checkpoint_path}")
        logger.info("\nğŸ’¡ Hint: Train a model first using: python train.py configs/lora_1.5b.yaml")
        sys.exit(1)
    
    if not Path(data_path).exists():
        logger.error(f"âŒ Data file not found: {data_path}")
        sys.exit(1)
    
    # åŠ è½½æ¨¡å‹
    logger.info("Loading model...")
    model, tokenizer = load_model_and_tokenizer(
        checkpoint_path,
        args.base_model
    )
    
    # åŠ è½½è¯„æµ‹æ•°æ®
    logger.info("Loading evaluation data...")
    eval_data = load_eval_data(data_path)
    
    # è¯„æµ‹
    results = evaluate(
        model=model,
        tokenizer=tokenizer,
        eval_data=eval_data,
        output_file=output_path,
        max_samples=args.max_samples
    )
    
    logger.info("\nâœ… Evaluation completed!")


if __name__ == "__main__":
    main()
