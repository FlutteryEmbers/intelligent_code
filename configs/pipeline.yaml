# ================================================================================================
# Pipeline Configuration - Intelligent Training Data Generation
# ================================================================================================
# 业务流程：Parse → Auto Module → QA Gen → Design Gen → Quality → Dedup → Split → Export
# ================================================================================================

# ==================== 1. Project & Repository Configuration ====================
# 项目基本信息和代码仓库配置
repo:
  path: "./repos/java/spring-ai"              # Path to Python repository (required)
  commit: ""                        # Git commit hash (empty = auto-detect from git)

# ==================== 2. Global Settings ====================
# 全局配置：随机种子、批处理大小等
global:
  seed: 42                          # Random seed for reproducibility (used by dedup, split, auto_requirements)
  batch_size: 5                     # Default batch size for generation
  top_k_context: 6                  # Default Top-K context selection for RAG

# ==================== Language Configuration ====================
# Language-specific rules for QA/Design generation and parser selection
language:
  name: "java"                      # Language name: java, python (determines parser and rules)
  profile_dir: "configs/language"   # Directory containing language profile YAML files

# ==================== 3. LLM Configuration (Core Dependency) ====================
# LLM是整个pipeline的核心依赖，所有生成步骤都依赖此配置
llm:
  provider: "ollama"
  base_url: "http://localhost:11434/v1"
  model: "qwen2.5:7b"
  temperature: 0.2  # 降低 temperature 以提高格式稳定性
  max_tokens: 10000  # 优化后：6个需求约需8000 tokens，留有余量
  timeout: 180  # 增加超时时间以适应更长的生成

# ================================================================================================
# PIPELINE STEPS (按执行顺序排列)
# ================================================================================================

# ==================== Step 1: Code Parsing ====================
# Code parsing configuration (now in language profile configs/language/{language}.yaml)
# You can override these settings here for project-specific customization

# Optional: Override language profile's parsing defaults
# filter:
#   ignore_paths:
#     - "custom_path"  # Additional paths to ignore (merged with profile)
#   file_extensions:
#     - ".java"  # Override profile's file extensions

# Optional: Override parser settings from profile
# parser:
#   max_chars_per_symbol: 15000  # Override profile's default
#   include_private: true        # Override profile's default
#   include_test: true           # Override profile's default

# Note: If not specified, all settings come from configs/language/{language}.yaml

# ==================== Step 2: Auto Module (Method Understanding & QA) ====================
# 自动理解方法并生成问答对
auto:
  enabled: true                                       # Enable auto module
  max_methods: 200                                    # Maximum methods to process (cost control)
  questions_per_method: 3                             # Number of questions per method
  top_k_context: 6                                    # Top-K methods for context retrieval
  embedding_model: "nomic-embed-text"                 # Ollama embedding model
  prompts:
    method_understanding: "configs/prompts/auto_method_understanding.txt"
    question_generation: "configs/prompts/auto_question_generation.txt"
    answer_generation: "configs/prompts/auto_answer_generation.txt"
  outputs:
    method_profiles_jsonl: "data/intermediate/method_profiles.jsonl"
    method_understanding_rejected_jsonl: "data/intermediate/auto_method_understanding_rejected.jsonl"
    questions_jsonl: "data/intermediate/questions.jsonl"
    embeddings_jsonl: "data/intermediate/method_embeddings.jsonl"
    auto_qa_raw_jsonl: "data/intermediate/auto_qa_raw.jsonl"
    auto_answer_rejected_jsonl: "data/intermediate/auto_answer_rejected.jsonl"

# ==================== Step 3a: Auto Requirements Generation ====================
# 基于代码自动生成架构改进需求
auto_requirements:
  enabled: true                    # true = auto-generate requirements, false = use configs/requirements.yaml
  max_requirements: 200               # Maximum requirements to generate (reduced for token efficiency)
  top_k_symbols: 12                 # Top-K symbols for context
  require_min_evidence: 2           # Minimum evidence_refs per requirement
  max_context_chars: 18000          # Maximum characters for context (prompt size control)
  use_method_profiles: true        # Use method profiles to enhance requirement generation
  method_profiles_jsonl: "data/intermediate/method_profiles.jsonl"  # Method profiles file path
  profiles_top_k: 20                # Top-K profiles for semantic enhancement
  profiles_max_chars: 6000          # Max characters for profiles context to prevent context explosion
  batching:                         # Batch generation to reduce JSON truncation risk
    enabled: true                   # Enable batch generation (false = single-shot like before)
    batch_size: 2                   # Generate N requirements per batch (smaller = safer)
    max_batches: 5                  # Maximum batch iterations (safety limit)
    diversity_hint: true            # Provide existing requirements to avoid duplicates
  prompts:
    requirement_generation: "configs/prompts/auto_requirement_generation.txt"
  outputs:
    requirements_jsonl: "data/intermediate/requirements_auto.jsonl"
    rejected_jsonl: "data/intermediate/requirements_auto_rejected.jsonl"

# ==================== Step 3b: QA Generation ====================
# 基于代码符号生成问答对
qa_generator:
  max_context_chars: 16000
  batch_size: 5
  max_samples: 50
  priority_annotations:
    - "Transactional"
    - "GetMapping"
    - "PostMapping"
    - "PutMapping"
    - "DeleteMapping"
    - "RequestMapping"
    - "Service"
    - "RestController"
    - "Controller"

# ==================== Step 3c: Design Generation ====================
# 基于需求生成设计方案
design_generator:
  top_k_context: 6
  max_context_chars: 20000
  max_samples: 10
  require_min_evidence: 2

# ================================================================================================
# DATA QUALITY & PROCESSING (数据质量控制和后处理)
# ================================================================================================

# ==================== Step 4: Quality Control ====================
# 数据质量检查和过滤
quality:
  min_instruction_length: 10
  min_answer_length: 20
  max_answer_length: 6000           # Match llm.max_tokens for consistency
  enable_deduplication: true

# ==================== Step 5: Deduplication ====================
# 去重处理，移除相似样本
dedup:
  simhash_bits: 64                  # Simhash fingerprint bits
  max_hamming: 3                    # Maximum hamming distance for near-duplicates

# ==================== Step 6: Train/Val/Test Split ====================
# 数据集划分
split:
  train_ratio: 0.8                  # Training set ratio
  val_ratio: 0.1                    # Validation set ratio
  test_ratio: 0.1                   # Test set ratio
  group_by: "package"               # Grouping strategy: "package" or "path"

# ==================== Step 7: Safety Scan ====================
# 安全扫描，检测敏感信息
safety:
  mode: "drop"                      # Mode: "drop" (remove samples) or "sanitize" (redact secrets)
  scan_enabled: true

# ==================== Step 8: Export ====================
# 导出最终数据集
# Note: system_prompt is now configured in language profiles (configs/language/*.yaml)
export:
  # No export-specific config needed, uses language profile settings

# ================================================================================================
# OUTPUT PATHS (输出路径配置)
# ================================================================================================

# ==================== QA Output Paths ====================
paths:
  # QA数据输出（问答对）
  qa_final_dir: "data/final/qa"
  qa_train_jsonl: "data/final/qa/train.jsonl"
  qa_val_jsonl: "data/final/qa/val.jsonl"
  qa_test_jsonl: "data/final/qa/test.jsonl"
  qa_train_sft_jsonl: "data/final/qa/train_sft.jsonl"
  qa_val_sft_jsonl: "data/final/qa/val_sft.jsonl"
  qa_test_sft_jsonl: "data/final/qa/test_sft.jsonl"
  
  # Design数据输出（设计方案）
  design_final_dir: "data/final/design"
  design_train_jsonl: "data/final/design/train.jsonl"
  design_val_jsonl: "data/final/design/val.jsonl"
  design_test_jsonl: "data/final/design/test.jsonl"
  design_train_sft_jsonl: "data/final/design/train_sft.jsonl"
  design_val_sft_jsonl: "data/final/design/val_sft.jsonl"
  design_test_sft_jsonl: "data/final/design/test_sft.jsonl"

# ==================== Directory Structure ====================
output:
  raw_extracted: "data/raw/extracted"           # 原始提取的符号数据
  raw_repo_meta: "data/raw/repo_meta"           # 仓库元数据
  intermediate: "data/intermediate"             # 中间处理结果
  final: "data/final"                           # 最终输出数据
  reports: "data/reports"                       # 分析报告

# ================================================================================================
# SYSTEM CONFIGURATION (系统配置)
# ================================================================================================

# ==================== Logging Configuration ====================
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/pipeline.log"